---
title: "Taking a Leap of Faith: Association to Causation"
author: "Lindsey Dietz, PhD"
date: "`r Sys.Date()`"
format: 
  revealjs:
    incremental: true  
    theme: dark
    code-fold: true
    scrollable: true
    progress: true
    fontsize: 2.2em
    slide-number: true
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Disclaimer

The views expressed in this presentation are strictly my own. They do not necessarily represent the position of the Federal Reserve Bank of Minneapolis or the Federal Reserve System.

## Who am I?

::: columns
::: {.column width="75%"}
-   [Data Science Manager \@ Federal Reserve Bank of Minneapolis](https://www.minneapolisfed.org/news-releases/2021/dietz-recognized-for-excellence-in-bank-supervision)
    -   I lead a team working on implementation and analysis of models for stress testing the largest banks.
-   R Enthusiast and STEM diversity advocate
    -   [Creator of the noRth R user conference](https://rnorthconference.github.io/)
    -   [Co-organizer of the R Ladies - Twin Cities meetup](https://www.meetup.com/rladies-tc/members/)
    -   [IF/THEN AAAS STEM Ambassador](https://www.si.edu/womensfutures)
:::

::: {.column width="25%"}
<center><img src="images/statue.png" width="500"/></center>
:::
:::

# Motivation

## Coffee

How many of you drink coffee?

. . .

How many of you think it is bad for you?

. . .

*"The **risk** of hypertension associated with coffee intake varies according to CYP1A2 genotype. Carriers of slow 1F allele are at **increased risk** and should thus abstain from coffee, whereas individuals with 1A/1A genotype can safely drink coffee."*

-- Journal of Hypertension, August 2009

## Coffee

How many of you drink coffee?

How many of you think it is good for you?

. . .

*"Drinking two to three cups of coffee a day is **linked with a longer lifespan and lower risk** of cardiovascular disease compared with avoiding coffee."*

-- European Journal of Preventive Cardiology, September 2022

##  {background-iframe="https://giphy.com/embed/UnTC9o2HMyUta" background-repeat="a\"repeat\""}

## Agenda

-   Causal Inference Preliminaries
-   Matching
-   Example using R and Airbnb Data

## Association To Causation

-   Most researchers want to understand more than just association (a.k.a links, risks, etc.)
-   For example,
    -   Will a new feature increase revenue in my app?
    -   Does a vaccine prevent the flu?
    -   Will a regulatory policy change impact bank capital?

## Causal Inference

Causal inference is a general set of principles and methods that allow us to make statistical inferences about the causal effects of treatments from randomized and observational data.

## 

*Did she just say I can make causal statements about observational data?*

<center><img src="https://media.giphy.com/media/5SXsDXvfhx2eI/giphy.gif" width="400"/></center>

. . .

Yes, I did, but...

## The 'Leap of Faith'

We must make **strong and mostly untestable** assumptions about the treatment assignment mechanism to go from descriptive to causal inference for observational data.

## Those who can, design experiments!

::: columns
::: {.column width="30%"}
<center><img src="https://media.giphy.com/media/vnELlPo3FyzfO/giphy.gif" width="500"/></center>
:::

::: {.column width="70%"}
-   Randomized Experiments a.k.a *Randomized Controlled Trials (RCTs)* or *A/B testing* are the gold standard for causation.
-   However, ethical or logistical reasons may prevent us from using experimentation.
    -   We cannot force people to do dangerous things.
    -   If one unit is influenced by another, we cannot parse treatment impacts.
:::
:::

# Causal Model

## Neyman--Rubin Causal Model

Each unit $i$ has *potential* outcomes $\{Y_{0i}, Y_{1i}\}$ before a treatment decision is made ($T_i \in \{0,1\}$. We want to understand the treatment effect: $\tau_i = Y_{1i} - Y_{0i}$.

![](images/outcomes.png){fig-align="center"}

## Fundamental problem of causal inference

However, we cannot observe both potential outcomes for a unit. A *counterfactual* outcome is what would have been observed if the treatment had been different.

![](images/observed.png){fig-align="center"}

## Consistency Assumption

<center><img src="https://media.giphy.com/media/fvT2tuQGmYxsQbSrQH/giphy.gif" width="1000"/></center>

## Consistency Assumption

-   English: Each outcome is observable.

-   Technical: $Y_i = Y_{ti}$ if $T_i = t$ for all $t$. This implies $Y_i = T_iY_{1i} + (1-T_i)Y_{0i}$.

## Causal effects of interest

The average treatment effect (ATE) is the average of all treatment potential outcomes less the average of all control potential outcomes.

```{=tex}
\begin{align}
{ATE} &= E[\tau_i] = E[Y_{1i} − Y_{0i}]
\end{align}
```
The average treatment effect for the treated (ATT) is the ATE for the treated group.

```{=tex}
\begin{align}
{ATT} &= E[\tau_i|T_i = 1] = E[(Y_{1i} − Y_{0i})|T_i = 1]   
\end{align}
```
The key idea in either of these is estimating the counterfactual.

## ATE in a Randomized Experiment

Treatment assignment is independent of potential outcomes, i.e. $Y_{i0}, Y_{i1} \perp T_i$. Thus,

```{=tex}
\begin{align}
E[Y_{1i} - Y_{0i}] & = E[Y_{1i}] - E[Y_{0i}] \\
& = E[Y_{1i}|T_i = 1] - E[Y_{0i}|T_i = 0] \textrm{ by independence}\\
& = E[Y_{i}|T_i = 1] - E[Y_{i}|T_i = 0] \textrm{ by consistency}  
\end{align}
```
## Positivity Assumption

<center><img src="https://media.giphy.com/media/zRwA2JgARLVYgWtfgY/giphy.gif" width="800"/></center>

## Stable Unit Treatment Value Assumption (SUTVA)

<center><img src="https://media.giphy.com/media/ailnj2AMt9e9i/giphy.gif" width="800"/></center>

## Stable Unit Treatment Value Assumption (SUTVA)

-   English: No interference, i.e. potential outcomes for unit $i$ are unaffected by the treatment assignment for unit $j$.

-   Probabilistic: $Y_{ti}|Y_{sj} = Y_{ti}$

-   Why: This simplifies the missing data problem. Consider if we had to estimate unit $i$ under every possible case of treatment assignment to any other subject.

## Confounders

A confounder (X) is a variable or set of variables that affects both the treatment and the outcome. If we control for confounders, we can still make inferences about the treatment impact on the outcome.

```{r dag}
#| message: false
#| echo: false
library(ggdag)
library(ggplot2)
theme_set(theme_dag())

smoking_ca_dag <- dagify(
  T ~ X,
  Y ~ T + X
)

ggdag(smoking_ca_dag, node_size = 20, stylized = TRUE, seed = 1013)

```

## Positivity Assumption

-   English: Conditional on confounders, every experimental unit is equally likely to be assigned a treatment.

-   Probabilistic: $P(T = t | X = x) > 0$.

-   Why: If some segment of the population has no chance of being assigned a treatment, there is no counterfactual to estimate.

## Ignorability Assumption

<center><img src="https://media.giphy.com/media/IdmfEtnMWPzOg/giphy.gif" width="1000"/></center>

## Ignorability Assumption

-   English: There are no unmeasured confounders.

-   Probabilistic: $Y_{0}, Y_{1} \perp T | X$.

-   Why: Among those with the same confounders, we can assume the treatment has been randomly assigned.

# Propensity Scores & Matching

## Propensity Scores

-   In a randomized experiment, $P(T=1|X) = P(T=1) = 0.5$.

-   In most observational data sets, there was no random assignment of the treatment $P(T=1|X) = f(X)$. However, we can estimate $f(X)$ with data and assumptions about which $X$ matter.

    -   Ex. a unit with predicted propensity score of 0.3 has a 30% chance of receiving treatment.

-   Once we match on propensity score, we can meet our assumption of *Ignorability*.

## Matching in Practice

1.  Fit a propensity score model for treatment: $P(T=1|X) = f(X)$
    -   Choice of model for a binary outcome such as logistic regression or machine learning alternatives
2.  Apply the model to predict a propensity score for each observation.
3.  Match observations based on a selected algorithm.
    -   Choice of distance to optimize
    -   Choice of maximum distance (caliper)
4.  Use matched outcomes in statistical inference

# R Example

## Airbnb Data Example

Will adding heating in Minneapolis-St. Paul Airbnb listings lead to increased revenue per stay?

-   Treatment: Heating present
-   Control: Heating absent
-   Unit: Unique listing
-   Outcome: Nightly cost
-   Plausible Confounders: reviews per month, \# of bedrooms, \# of bathrooms, type of listing (Private Room vs. Entire home).

. . .

*Please note there are many many other confounders possible that I'm not including for simplicity.*

## Setting up libraries and reading data

```{r dataclean}
#| message: false
#| code-fold: false

library(dplyr)
library(tidyr)
library(stringr)
library(MatchIt)
library(tableone)
library(data.table)
library(ggplot2)

# Custom functions for this analysis
source('support_functions.R')

# http://insideairbnb.com/get-the-data
msp <- reading_data(url = "http://data.insideairbnb.com/united-states/mn/twin-cities-msa/2022-09-16/data/listings.csv.gz")

```

## Data cleaning and review

```{r}
#| message: false

# Custom function to get top n amenities for simplicity
top40msp <- top_amenities(data = msp, number = 40) 

# Custom function to apply filters and pre-processing
analysis_set_msp <- create_analysis_set(
                      data = msp, 
                      amenities_vector = top40msp, 
                      test_var = 'Heating',
                      inactive_date = as.Date('2022-04-30'),
                      room_types = c('Entire home/apt', 'Private room'),
                      standard_baths = 1:3,
                      standard_beds = 0:5,
                      review_lower_bound = 0,
                      min_nights_upper_bound = 7)

unmatched_tbl <- tableone::CreateTableOne(
                         vars = c('reviews_per_month', 'beds', 'bathroom', 'room_type'),
                         data = analysis_set_msp, 
                         strata = 'Heating', 
                         smd = TRUE, 
                         test = FALSE)

print(unmatched_tbl, smd = TRUE)

```

## Standardized Mean Difference (SMD)

SMD is a measure of distance between two group means.

-   Continuous variables: $SMD = \frac{\bar{x}_t - \bar{x}_c}{\sqrt{\frac{s_t^2 + s_c^2}{2}}}$

-   Binary variables: $SMD = \frac{\hat{p}_t - \hat{p}_c}{\sqrt{\frac{p_t(1-p_t) + p_c(1-p_c)}{2}}}$

-   Heuristic: SMD \< 0.1 indicates a negligible difference in the mean or prevalence of a covariate between groups.

::: fragment
```{r}
#| message: false
#| echo: false
print(unmatched_tbl, smd = TRUE)

```
:::

## Fit a propensity score model

```{r propscore2}
#| message: false
#| cache: true

matched_out <- MatchIt::matchit(Heating ~ reviews_per_month + factor(beds) +
                                  factor(bathroom) + factor(room_type), 
                                data = analysis_set_msp, 
                                distance = 'glm',
                                link = 'logit',
                                method = 'optimal',
                                caliper = NULL)

```

. . .

Some variations to consider:

-   Replace method with one of several others: "nearest" will give nearest neighbor a.k.a greedy matching

-   Use a caliper to reduce possible distances eligible for matching

## Matched outputs

```{r propscorematch}
#| message: false
#| cache: true

final_match <- MatchIt::match.data(matched_out) %>% 
  dplyr::arrange(subclass, desc(Heating)) %>%
  dplyr::rename(prop_score = distance) 
  
final_match %>%
  dplyr::select(subclass, prop_score, price, Heating, beds, bathroom)
```

## Review balance of matching

```{r propscore4}
#| message: false

match_tbl <- tableone::CreateTableOne(
                    vars = c('reviews_per_month', 'beds', 'bathroom', 'room_type'), 
                    data = final_match,
                    strata = 'Heating',
                    smd = TRUE, 
                    test = FALSE)

print(match_tbl, smd = TRUE)

```

SMDs are \< 0.1 which means we can move forward.

## Check positivity assumption

```{r propscore31}
#| message: false

# Positivity assumption checking
# Is there overlap between propensity scores in control and treated?
ggplot2::ggplot(final_match, aes(x = prop_score)) +
  geom_density(data = final_match %>% 
                 filter(Heating == TRUE), 
               aes(x = prop_score, y = ..density..), fill= "#69b3a2") +
  geom_density(data = final_match %>% 
                 filter(Heating == FALSE), 
               aes(x = prop_score, y = -..density..), fill= "#404080") +
  theme_minimal() +
  xlab('Propensity Score') +
  ylab('Density')
```

Looks good; we have overlap across the distribution.

## Run statistical inference

```{r propscore5}
#| message: false

# Vector of prices for heating (treatment) group
y_trt <- final_match %>% 
  dplyr::filter(Heating == TRUE) %>% 
  dplyr::pull(price)

# Vector of prices for heating (control) group
y_con <- final_match %>% 
  dplyr::filter(Heating == FALSE) %>% 
  dplyr::pull(price)

t.test(y_trt, y_con, paired = TRUE, 
       alternative = 'two.sided',
       mu = 0, 
       var.equal = FALSE,
       conf.level = 0.95)

```
